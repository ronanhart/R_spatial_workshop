--- 
title: "R Spatial Workshop"
author: "Ronan Hart"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::gitbook,
  set in the _output.yml file.
link-citations: yes
github-repo: ronanhart/R_spatial_workshop
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
p.caption {
  font-size: 0.7em;
}
</style>

# Welcome to the Ecology Center's Workshop on R Spatial! 

In this workshop we will cover: 

* GIS Basics
* Vectors & Rasters and how to manipulate them in R
* Basic GIS functions
* (Briefly) where to obtain environmental GIS data

We will **NOT** be covering how to make a nice-looking map (for figures, presentations, etc.) That will be covered in the next workshop, so be sure to keep an eye out for that if you want to learn how to make maps in R!

---------------------------------------------------------------------------------

**Goals of this workshop:**

* Reiterate the basics of GIS
* Teach you the processes that you might have learned in ArcGIS or other GIS software that you can code yourself in R

## Why GIS in R? {-}

You may be asking why you even need to learn how to code spatial processes in R, especially if you already know how to use ArcGIS. (Well, maybe you're not actually asking that question if you’re taking this class.) But here are a few reasons why:

* Free
  + you most often need to pay companies to use their GIS software (or be a           student to use a university's license). What happens when you're no longer a      student nor hired by a company/organization that already has a license?
* Reproducible
  + Some journals require you to publish your code alongside your manuscript
  + If you are collaborating on a project, you can 
* Open-Source
  + company-owned software is often hidden behind a "black box" so you might not be 100% certain what a function or tool is doing. 
  + Functions in R are completely open to the public so you can be certain that a function is doing what you think it's doing
* Customizable
  + You can write your code to suit your specific problem and answer your specific questions 
  + You can write custom functions and loops so that you can repeat processes on multiple features and rasters without having to copy and paste code 

Reproducibility and customization are not unique to R but rather an advantage to using code for GIS in general. In a few weeks there will be a workshop on coding in python for GIS tools, which is just as useful (especially because you can use the package `arcpy` in python to code directly to ArcGIS), so I also recommend taking that workshop if you’re interested.

Not to say that programs such as ArcGIS should never be used. On the contrary, since it was the way I first learned GIS, I will sometimes return to it to make a map on the fly or quickly visualize and double-check a polygon or raster. All programs have their pros and cons, and this workshop is to simply add another tool in your spatial analysis toolbox. 

---------------------------------------------------------------------------------

## Prerequisites, Packages, and Preparation {-}

Before we begin, please be sure you have done the follow preparations:

1. Make sure you have a recent version of [R](https://cran.r-project.org/) and [RStudio](https://www.rstudio.com/) installed
  + to check your version of R, type `R.version` in the console. `version.string` will have the version number and its release date. The most recent version (as of 2022-03-30) is 4.1.3 (released on 2022-03-10). **Version 4.1.2 is perfectly fine for this workshop.**
  + to check your version of RStudio, Go to Help (in the toolbar at the top) > Check for Updates. RStudio 2022.02.1+461 is the most recent version. **Version 2021.09.2+382 is perfectly fine for this workshop.**
  
```{r help, fig.align = 'center', out.width = '25%', echo = F}
knitr::include_graphics("pictures/help.png")
```

2. Install (if needed) and load the following packages:

```{r installDemo, echo = T, eval = F}
install.packages("raster")
install.packages("rgdal")
install.packages("rgeos")
install.packages("sf")
install.packages("sp")
install.packages("tidyverse")
```

```{r mapsPkg, echo = F, eval = T, message = F}
library(maps)
```

```{r packages, echo = T, eval = T, message = F}
library(raster)
library(rgdal)
library(rgeos)
library(sf)
library(sp)
library(tidyverse)
```

3. (Optional but recommended) Create a new RStudio project. This will make it easier to access files within your code and keep everything organized. [Here's a guide on how to do that](https://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects)

After taking care of that, let's get started!

<!--chapter:end:index.Rmd-->

# GIS basics

I apologize if you've taken a GIS course or two and are already familiar with these concepts. I want to make sure we're all on the same page and that we all know how these concepts work in R.

## Datums, Projections, and Coordinate Systems

### Datums {-}

The Earth is a **spheroid** (also called an **ellipsoid**). Because of variations in elevation across the world, the Earth's surface is irregular.

```{r earthSpheroid, fig.cap = "Conceptual representation of the irregular, spheroid shape of the Earth", fig.align = 'center', out.width = '25%', echo = F}
knitr::include_graphics("pictures/spheroid.png")
```

A **datum** (also called a **geographic coordinate system**) is a reference surface that best fits the mean surface area of an area of interest. There is a global datum to represent the general surface of the Earth as a whole --- World Geodetic System of 1984 i.e. WGS84.

```{r wgs, fig.cap = "Red ellipse represents the smooth, general surface of the Earth (i.e. a global datum)", fig.align = 'center', out.width = '25%', echo = F}
knitr::include_graphics("pictures/wgs.png")
```

   
However, because the Earth's surface is irregular and the global datum might not reflect specific areas and variations in elevations, there are also local datums. A common local datum (for North America at least) is the North America Datum of 1983 (NAD83) . 

```{r local, fig.cap = "Yellow line indicates a specific area, purple ellipse represents the smooth, general surface of the Earth at this location. Note that this local datum would not be a best fit in other places on the Earth", fig.align = 'center', out.width = '25%', echo = F}
knitr::include_graphics("pictures/local.png")
```

 
   
The datum you choose to work with is up to you and where your study takes place. It is **very** important to know what datum you're working with and that you remain consistent because coordinates of a location from one datum are likely different than the same location from a different datum. For example, if we look at the coordinates for Bellingham, Washington:

  Datum  |     Longitude    |	Latitude
---------| ---------------- | --------------------
NAD 1983 | -122.46818353793 | 48.74387985**43649**
WGS 1984 | -122.46818353793 | 48.74387985**34299**

While the differences between NAD83 and WGS84 are not huge, these differences could impact any spatial analysis you perform. Also note that you would need to choose a different local datum if you're working outside of North America.
 

### Projections {-}

While a datum references the position of an object in geographic space on a 3D surface, a **projection** (also called a **projected coordinate system**) represents that 3D surface onto a 2D plane. 

```{r project, fig.cap = "Conceptual demonstration of map projections", fig.align = 'center', out.width = '50%', echo = F}
knitr::include_graphics("pictures/projection.png")
```
  
This is important to know when plotting a map for a figure, as your chosen projection will change the visualization and shape of your map's features. But more importantly for spatial analysis, a projection is needed when you need values such as length, area, or distance. Map projections are never 100% accurate, so every 2D map will have show some distortion. Different projections preserve different properties of the world, such as the relative shape of features, area, distance, or angle. For that reason, it's important to pick a projection that would provide the highest accuracy for your region and the analysis you're running. 

A common projection to use is the **Universal Transverse Mercator** or **UTM**. 

```{r utm, fig.cap = "UTM around the globe", fig.align = 'center', out.width = '50%', echo = F}
knitr::include_graphics("pictures/utm-1024x512.jpg")
```

```{r utmUSA, fig.cap = "UTM for the US", fig.align = 'center', out.width = '50%', echo = F}
knitr::include_graphics("pictures/utm_usa.png")
```

If your study region is in Utah, for example, you would use UTM Zone 12 N (or UTM 12N).

Note that while you will **always** have a datum, **you do not necessarily need to ALWAYS use a projection.** As for anything, it depends on your analysis and your system.



### Coordinate Reference System {-}

A **coordinate reference system** or **CRS** is simply the combination of the datum (geographic coordinate system) and the projection (projected coordinate system). For example, if you are working with the 1984 World Geodetic System that is projected to UTM Zone 12N, your CRS would be WGS84 UTM12N. If you are working with the 1983 North America Datum that is projected to UTM Zone 14N, your CRS would be NAD83 UTM14N. And so on.

These different combinations of CRS all have their own **EPSG code.** (These codes were orginally created by the European Petroleum Survey Group, which is where the acronym comes from). 

For example, the EPSG code for WGS84 latitude/longitude (i.e. no projection) is **4326**, the EPSG code for NAD83 UTM12N is **26912**, and so on. These codes can easily be found on [the Spatial Reference Website](https://www.spatialreference.org/) (or google if you forget what the website is).

---------------------------------------------------------------------------------

## Spatial Data

### Vectors {-}

Vector data are shapes with a geometry that can represent real world features. These geometries which can be made up of one or more **vertices and paths**. A vertex describes a position in space with x and y coordinates. A feature with one vertex would be a **point**, a feature with two or more vertices where the first and last vertices don't connect would be a **polyline**, and a feature with at least three vertices and the first and last vertices connect (an enclosed area) would be **polygon**.

* Points
  + animal positional locations
  + study site coordinates
  + tree locations
* Lines
  + roads
  + fences
  + boundaries
  + rivers
* Areas (or polygons)
  + bodies of water
  + parks 
  + USFS land
  + study plots
  + area burned by a fire

Example with random vertices:

```{r features, echo = F, eval = T, fig.cap = "Figure demonstrating points (red), polylines (black), and polygon (blue)", fig.align = 'center'}

set.seed(1)

plygn <- matrix(runif(8), ncol = 2)
plygn[4,] <- plygn[1,]
plygn <- Polygon(plygn)
plygn <- Polygons(list(plygn), ID = "area")
plygn <- SpatialPolygons(list(plygn))

ln <- Line(matrix(runif(10, min(bbox(plygn), max(bbox(plygn)))), ncol = 2))
ln <- Lines(list(ln), ID = c("ln1")) 
ln <- SpatialLines(list(ln))

pts <- SpatialPoints(matrix(runif(10, min(bbox(plygn), max(bbox(plygn)))), ncol = 2))

plot(plygn, col = "lightblue", border = "lightblue")
lines(ln, lwd = 2)
points(pts, pch = 16, cex = 1, col = "darkred")
```

Or an example with Utah features:

```{r utahExample, eval = T, echo = F, fig.cap = "Figure demonstrating points (major Utah cities), polylines (major Utah highways), and polygons (shape of Utah boundary)", fig.align = 'center'}

utah <- maps::map("state", plot = F, fill = TRUE) %>%
  # turn into sf obj
  sf::st_as_sf() %>%
  # pull out utah 
  dplyr::filter(ID == "utah")

ut_city <- us.cities %>%
  dplyr::filter(country.etc == "UT") %>%
  sf::st_as_sf(coords = c("long", "lat"), crs = 4326)

fwy <- st_read("Data/Examples/utah_freeway", "utah_freeway", quiet = T) %>%
  st_transform(crs = 4326)

p <- ggplot() +
  geom_sf(data = utah) +
  geom_sf(data = ut_city) +
  geom_sf(data = fwy) +
  theme_void()
p
```

Vector features have attributes, which can be text or numerical information that describe the features. These attributes are stored in a data frame. 

```{r attribute, echo = F, eval = T}

ut_city <- us.cities %>%
  dplyr::filter(country.etc == "UT") %>%
  rename(state = country.etc) %>%
  mutate(capital = ifelse(!(capital == 0), 1, 0))
print(ut_city)

```

The important information you need from your vector data are: 

* the geometry type (if it's a point, line, or polygon)
* the coordinate reference system
* the bounding box (the min/max points in x and y geographical space)


### Rasters {-}

Rasters are data represented by **pixels** (or **cells**) where each pixel has its own value. These cell values can be **continuous** (e.g. elevation, temperature, snow depth) or **discrete** (e.g. land cover, habitat type, presence/absence).

```{r contDemo, eval = T, echo = F, fig.cap = "Map showing a raster with continuous values (elevation)", fig.align = 'center'}
dem <- raster("Data/Examples/elevation.tif")
dem %>%
  as.data.frame(xy = T) %>%
  ggplot(aes(x = x, y = y, fill = elevation)) + 
  geom_raster() +
  scale_fill_gradient(low = "grey0", high = "grey100", na.value = "transparent") +
  labs(fill = "Elevation (m)")
```

```{r discDemo, eval = T, echo = F, fig.cap = "Map showing a raster with discrete values (land cover)", fig.align = 'center'}
land <- raster("Data/demo/landcover_demo.tif")
land %>%
  as.data.frame(xy = T) %>%
  mutate(Landcover = as.factor(case_when(landcover_demo %in% c(138:184) ~ 1,
                                         landcover_demo %in% c(312:445) ~ 2,
                                         landcover_demo %in% c(490:495) ~ 3,
                                         landcover_demo %in% c(529) ~ 4,
                                         landcover_demo %in% c(554) ~ 5,
                                         landcover_demo %in% c(557) ~ 6,
                                         landcover_demo %in% c(579) ~ 7,
                                         landcover_demo %in% c(581:582) ~ 8))) %>%
  ggplot(aes(x = x, y = y, fill = Landcover)) + 
  geom_raster() +
  scale_fill_manual(values = c("darkgreen", "darkolivegreen4", "burlywood3", "antiquewhite4", "azure4",
                               "gold", "deepskyblue", "darkred"),
                    labels = c("Forest & Woodland", "Shrub & Herb Vegetation", "Desert & Semi-Desert",
                               "Open Rock Vegetation", "Nonvascular & Sparse Vascular Rock Vegetation",
                               "Agricultural & Developed Vegetation", "Open Water", "Developed & Other Human Use"))
```

Raster data can have more than one **band** (where each band is a single raster). These raster layers can stack together to create a **Raster Stack** or a **Raster Brick** (the differences between these two are minor and we will go into more depth later). For example, satellite imagery is a stack of 3 rasters, each containing continuous values indicating levels of Red, Green, and Blue. These 3 bands come together to make a true-color image. 

The important information you need from your raster data are

* the coordinate reference system
* the extent (the min/max points in x and y geographical space)
* the cell resolution (the width and height of each cell)

The cell resolution basically means how "pixel-y" the raster is. A **finer** resolution (meaning the cell size is smaller) will have more detail than a **coarser** resolution (meaning the cell size is larger). For example, compare a raster with a pretty fine resolution (in this case 30m X 30m, meaning that each cells is 30m wide and 30m high)

```{r fine, eval = T, echo = F, fig.cap = "Map showing a raster with fine resolution (30m X 30m)", fig.align = 'center'}
dem %>%
  as.data.frame(xy = T) %>%
  ggplot(aes(x = x, y = y, fill = elevation)) + 
  geom_raster() +
  scale_fill_gradient(low = "grey0", high = "grey100") +
  labs(fill = "Elevation (m)")
```

Compared to the same raster but with a coarser resolution (in this case 300 m X 300 m)

```{r coarse, eval = T, echo = F, fig.cap = "Map showing a raster with coarse resolution (300 m X 300 m)", fig.align = 'center'}
dem_coarse <- aggregate(dem, fact = 10)
dem_coarse %>%
  as.data.frame(xy = T) %>%
  ggplot(aes(x = x, y = y, fill = elevation)) + 
  geom_raster() +
  scale_fill_gradient(low = "grey0", high = "grey100") +
  labs(fill = "Elevation (m)")
```

**Wouldn't we always want to work with finer resolutions?** If rasters with finer resolutions have more detail (and thus are more accurate to what's actually on the landscape) than one with a coarser resolution, why would we ever work with a raster with coarse resolution? I can think of 2 reasons why:

1. Sometimes you simply can't obtain that data in a finer resolution. For example, [MODIS](https://modis.gsfc.nasa.gov/data/dataprod/mod13.php) offers NDVI rasters every 16 days, but the finest resolution is 250m.
2. The finer the resolution, the more cells there are, and so the time to do any sort of computation or analysis on these cells increases.

As for everything, it depends on your analysis and your system.

## Vectors vs Rasters: pros & cons

**Advantages of Vectors**
 
* Because vectors are just vertices and paths (rather than upwards of thousands of grid cells), it takes less time to load, save, or perform any computation or analysis on a vector compared to a raster. (They also take up less disk space on your hard drive)
* For the same reason, they can often be more geographically accurate. A vector's vertex is located at a single lat/long coordinate compared to a raster pixel at the same location but covers 250mX250m.

**Disadvantages of Vectors**

* It is difficult to store and display continuous data in vectors. (It can be done, but the data typically would need to be binned)
* Vectors are best used to represent **features** of the landscape, rather than **the landscape itself**.

**Advantages of Rasters**

* Rasters are best for satellite and other remotely sensed data. As the point above mentioned, they are great for representing the landscape itself.
* It is relatively easy and intuitive to perform any quantitative analysis with rasters. When raster cells are stacked (see figure below), it is pretty straightforward to perform any focal statistics or cell algebra.

```{r stack, fig.cap = "A stack of rasters, showing how each cell would correspond to the ones on top and below", fig.align = 'center', out.width = '50%', echo = F}
knitr::include_graphics("pictures/raster_stack.png")
```

**Disadvantages of Rasters**

* Depending on the resolution, they can look pixellated and not visually appealing. For analysis, this would affect computation time and disk space.
* Raster cells can only contain **one** value (compared to vectors, which can have an entire attribute table). If you want cells to contain more than one value, you would need a stack of rasters, which takes up disk space and computation power.

---------------------------------------------------------------------------------

Now that we know about coordinate reference systems, vectors, and rasters, let's learn how to deal with all of these in R!

<!--chapter:end:01-intro_GIS-basics.Rmd-->

# Spatial Data in R

## Vectors 

The primary packages that deal with these features are [`sf`](https://cran.r-project.org/web/packages/sf/index.html) and [`sp`](https://cran.r-project.org/web/packages/sp/index.html)

**sf** means "simple feature" while **sp** is short for "spatial." There are pros and cons to both (for example, a lot of other packages depend on `sp`, `sf` works well in `tidyverse`, etc.). However, in my opinion, there is no reason that you can't use both in the same code, if needed. [This website](https://cengel.github.io/R-spatial/intro.html#conceptualizing-spatial-vector-objects-in-r) goes into more detail about these packages and the specifics about objects from these packages. 

### Loading vector data from a spreadsheet

You will often have a spreadsheet of data that have two columns of latitude and longitude (and potentially other columns for the attributes of these features). 

```{r loadCSV, echo = T, eval = T}
sites <- read.csv("Data/Examples/Sites.csv")
head(sites)
class(sites)
```

How can we convert this data table into a spatial object? 

#### With sf {-}

We'll use the function `st_as_sf()` to convert an object of another class to an `sf` object. As with any function, you can always type `?st_as_sf` in the console to remind yourself what the function does and what arguments you need to add. In this case, the non-optional arguments are `x` (the object we want to convert, in this case our sites data frame) and `coords` (the columns in the data frame that have the coordinate data). 

The argument `crs` is optional but we should add it so that our spatial object has a coordinate reference system. Because our coordinates are lat/long, we should tell `st_as_sf` that our crs is WGS84 lat/long (Remember that the EPSG code for WGS84 lat/long is [4326](https://www.spatialreference.org/ref/epsg/4326/)). **Note: Even if you're working in a different geographic or projection coordinate system, it is ALWAYS good practice to load in lat/long coordinates as WGS84 lat/long and then later projecting the object, instead of right away giving it a projected coordinate system.** This is because GPS coordinates work with a global datum (being a **global** positioning system), and you might have inaccurate positions if you load it in with another datum or projection.

```{r stAsSf, eval = T, echo = T}
sites_sf <- st_as_sf(sites, 
                     coords = c("Longitude", "Latitude"), 
                     crs = 4326)
head(sites_sf)
```

Now when we look at the `head` of `sites_sf` it also prints out some information about this spatial object: it is a simple feature, it is a `POINT` object, the min/max coordinates (the bounding box), and the CRS. Note that the "Longitude" and "Latitude" columns turned into a "geometry" column. We can also check this information with separate functions: `st_bbox` and `st_crs`

```{r checkSFinfo, eval = T, echo = T}
st_bbox(sites_sf)

st_crs(sites_sf)
```

`st_crs` printed out a lot of information, but the important things to note are that we (the user) inputted EPSG: 4326, its datum is "World Geodetic System 1984" (or WGS 84), its axes are latitude and longitude, and its units are "degree" (as opposed to a linear unit like m that we might see in a projected crs). It's always good practice to double check the CRS of your object before you perform any analysis. 

Note that now when we look at the `class` of `sites_sf` it has **two** classes: `sf` and `data.frame`.

```{r classSF, eval = T, eval = T}
class(sites_sf)
```

This is one of the great things about `sf`: because `sf` objects remain as class `data.frame`, it makes it easy to work with an `sf` object like you would with any data frame.

Now that it's a spatial object, let's plot it and see what it looks like!

```{r plotBase, eval = T, echo = T}
plot(sites_sf, pch = 16)
```

This object only has one attribute ("Site"). If it had more than one, plotting in base R (i.e. using the `plot` function) plots all attributes (it will actually only plot 10 and will give you a warning if there are more than 10 attributes).

To just plot the feature's shape, we can use `st_geometry` to extract the geometry of the vector

```{r plotBase2, eval = T, echo = T}
plot(st_geometry(sites_sf), pch = 16)
```

We can also use `ggplot2` to plot `sf` objects. `ggplot2` has a function called `geom_sf`, which makes it easy to visualize `sf` objects with `ggplot2`

```{r sfGGplot, eval = T, echo = T}
ggplot(sites_sf) +
  geom_sf() # note that we don't need to tell ggplot what it should consider x and y to be; that's already built in with the "geometry" column!
```

Remember, though, that this workshop will not cover how to make nice maps in R (that will be in the next workshop). I just bring this up because you would want to be sure the data you're working with looks like what you might expect it to, so you would want to make a quick plot before moving on to analysis.

#### With `sp` {-}

Let's do the same thing but now with the `sp` package. We'll use the function `coordinates`. This function will pull the coordinates from an object of class `Spatial`, but it will also assign coordinates to an object of class `data.frame` and automatically convert it to a `Spatial` object by following this format:

```{r coordinatesEx, echo = T, eval = F}
coordinates(my_dataframe) <- c("x", "y") # x & y are the NAMES of the columns
# OR
coordinates(my_dataframe) <- c(1, 2) # 1 & 2 are the INDICES of the columns
```

I'm first going to make a duplicate object of "sites" called "sites_sp" because `coordinates` will overwrite the original `data.frame` object into a `Spatial` object (which is not necessarily a bad thing, but sometimes you want to keep all your objects separate in case you need to use the original data frame for some reason)

```{r sp, echo = T, eval = T}
sites_sp <- sites
coordinates(sites_sp) <- c("Longitude", "Latitude")
sites_sp
```

Just like with `sf`, printing the object in `sp` will also show some information about that object, namely the class, extent, and CRS. And just like with `sf` we can use functions to look at this information for a `Spatial` object: `class`, `bbox`, and `proj4string`

```{r checkSPinfo, eval = T, echo = T}
class(sites_sp)

bbox(sites_sp)

proj4string(sites_sp)
```

The CRS right now is `NA`. With `sf` we were able to assign a CRS while also converting the data frame to a simple feature, but with `sp` we need to do that in a separate step. We'll use `proj4string` again as this function not only checks the CRS of a spatial object but can also be used to assign a CRS. In `sf`,  we could give the function the EPSG code itself, but in `sp` we need to give it a character string wrapped in the function `CRS`. There are a few ways to write this character string, but the easiest one to remember, in my opinion, follows this format: `"+init=epsg:####"` (where #### would be the EPSG code)

```{r assignCRS, eval = T, echo = T, warning = FALSE}
proj4string(sites_sp) <- CRS("+init=epsg:4326")
proj4string(sites_sp)
```

Great! Now let's plot it to make sure it looks like what we would expect it to

```{r plotSP, eval = T, echo = T}
plot(sites_sp, 
     pch = 16) # it defaults, for some reason, to a cross shape, so this changes it to a filled in circle
```

It is not as straightforward to plot an object of class `Spatial` with ggplot2 as it was with `sf`, so I would recommend either converting the object to `sf` (which we will cover later in this chapter) or just plotting from the data frame

```{r pointsGGplot, eval = T, echo = T}
ggplot(sites) +
  geom_point(aes(x = Longitude, y = Latitude))
```

### Loading vector data

A lot of the time, you will have spatial data already saved as a shapefile. How do we load that into R?

#### with sf {-}

We'll use the function `st_read` which takes two arguments: `dsn` (data source name), which is essentially the folder where the shapefile is located, and `layer`, which is the name of file (without any extensions). `layer` is technically optional because `dsn` will choose the first file in that folder, so if the shapefile is the only file in that folder, then `dsn` will automatically choose the file. But I like to specify the layer name to avoid any mishaps.

```{r stRead, echo = T, eval = T}
# Shapefile: freeways in Utah
fwy_sf <- st_read(dsn = "Data/Examples/utah_freeway", layer = "utah_freeway")
```
You'll note that as we read in a shapefile with `st_read`, it automatically prints out the information about this feature (if you don't want R to print this, put `quiet = FALSE` in the function).

Again, we can check this information separately (in case we forget or we need to check later in the analysis). We can also check the first few rows of the data frame to see what kind of attributes there are, and we can plot the feature to make sure it looks like what we expect it to be.

```{r checkSF2, echo = T, eval = T}
st_bbox(fwy_sf)
st_crs(fwy_sf)
head(fwy_sf)
ggplot(fwy_sf) +
  geom_sf()
```

#### with sp {-}

To read in spatial data as an object in the `Spatial` family, we need to use the `rgdal` package. (When you load in `rgdal`, it will also automatically load in `sp`). We'll use the `rgdal` function `readOGR`. This function is similar to `st_read` in that its arguments are `dsn` and `layer` which work exactly the same way as they do in `st_read`

```{r readOGR, eval = T, echo = T}
fwy_sp <- readOGR(dsn = "Data/Examples/utah_freeway", layer = "utah_freeway")
```

`read_OGR`, also like `st_read`, will print out some information after reading in the shapefile. If you want to turn this off, you can add `verbose = FALSE` in the function's arguments.

Let's check the feature's information (class, extent, and CRS) and plot it

```{r checkSP2, eval = T, echo = T, warning = F}
class(fwy_sp)
bbox(fwy_sp)
proj4string(fwy_sp)
plot(fwy_sp)
```

### Projecting vector data

So far all of our vector data has only been in WGS 84 lat/long CRS. More than likely you will want to work in a projected coordinate system. So how do we re-project (or transform) features in R?

* In `sf` we'll use the function `st_transform()`
* In `sp` we'll use the function `spTransform()`

Both of these functions need the object you want to re-project and the target CRS. 

What CRS should we work in? Like I said earlier, UTM is a popular projection because it's localized and allows you to work with with linear metrics like length, area, and distance. Since both of these features (our site data and Utah freeways) are located in Utah, we would use UTM 12N. We can stay in the WGS 84 datum, but for the purposes of demonstration, let's change to NAD83. (Remember from the previous chapter that there is not much difference between these datums, so it's up to you if you want to use a more localized datum (NAD83) or a more recent datum (WGS84)). The EPSG code for NAD83 UTM 12N is [26912](https://www.spatialreference.org/ref/epsg/26912/). (If you want to work in WGS84 UTM 12N, the EPSG code is [32612](https://www.spatialreference.org/ref/epsg/32612/))

```{r transform, echo = T, eval = T, warning = F}
# sf
fwy_sf_proj <- st_transform(fwy_sf, crs = 26912)
st_crs(fwy_sf_proj)

# sp
sites_sp_proj <- spTransform(sites_sp, "+init=epsg:26912")
proj4string(sites_sp_proj)
```

Note that the coordinates of the WGS84 object compared the coordinates of the projected object are different.

```{r checkCoords, echo = T, eval = T}
st_geometry(fwy_sf)
st_geometry(fwy_sf_proj)
```

The coordinates for the object in WGS84 are in lat/long and the units are decimal degrees. The coordiantes for the projected object are in UTM and the untis are meters. 

**REMINDER:** You should **always** double-check the CRS of every feature and object you're working with because you want to make sure everything is projected to the **same** CRS! If they're not, it could result in inaccurate analysis and plotting. Depending on what functions you're using, R may give you a warning or error that the CRS of one feature and another don't match, but it's best not to rely on this and just double-check yourself.

### Saving vector data

Once you've created or modified a spatial object, most likely you want to save it so you can use it later or share with collaborators. There are many files types that can hold spatial data, but the most commonly used are ESRI Shapefile (which can be used in ArcGIS software), KML (which can be used with Google Earth Engine), GeoJSON, and PostgreSQL. If you're curious what other file types are out there, you can use functions `st_drivers()` or `ogrDrivers()` for `sf` and `sp` respectively. In this workshop, I will focus just on ESRI Shapefiles.

If you've worked with shapefiles before, you've likely noticed that a shapefile is actually a collection of files with the extensions .dbf, .prj, .shp, and .shx. It's often easier to organize and share shapefiles if they're in their own folder. If you don't already have a folder ready for any shapefiles that you're ready to save, you can, of course, manually make one in the File Explorer. But you can also create folders in R! I, personally, often like to do this because it helps streamline my process. To do so we'll use the functions `dir.exists()` to check if the directory (or folder) or not, and if it doesn't we'll use `dir.create()` to create a directory.

```{r saveData, echo = T, eval = F}
out_dir <- "Data/Examples/Sites_shp" # name what you want the folder to be and save it in an object

if(!dir.exists(out_dir)){ # this if statement is basically saying "if out_dir does NOT exist..." (the NOT is from the exclamation mark)
  dir.create(out_dir) # and if out_dir does NOT exist, then dir.create will create it
}
```

Now that we've created a new directory for this shapefile to go to, let's save our shapefile! For `sf` objects we'll use the function `st_write` and for `Spatial*` objects we'll use the function `writeOGR`. The arguments for these two functions are basically the same: they take the object you're saving, the dsn (or the folder) you want to save it to, the layer (or the name you want to save the file as, do NOT add an extension), and the driver (meaning if you're saving it as an ESRI Shapefile, KML, etc.)

```{r save, eval = F, echo = T}
# sf
st_write(sites_sf, dsn = out_dir, layer = "Sites_sf", driver = "ESRI Shapefile")

# sp
writeOGR(sites_sp, dsn = out_dir, layer = "Sites_sp", driver = "ESRI Shapefile")
```

In reality I would only be working with either an `sf` object or a `Spatial*` object and I wouldn't save both (because once they're saved as a shapefile, they are exactly the same file).

### Converting between `sf` and `sp`

As I mentioned before, there is no reason to not use both `sf` and `Spatial*` objects in one piece of code, but it doesn't make sense to load or create a single object as both an `sf` and `Spatial*` class. In some cases it makes more sense to work with `sf` objects and then convert them to `Spatial*` if needed (or vice versa). Lucky for us, it's pretty easy to convert an `sf` object to `Spatial*` and then back to `sf`. For `sf` -> `Spatial*` we'll use the function `as()` or `as_Spatial()`. For `Spatial*` -> `sf` we'll use a function we've allready used: `st_as_sf()`

```{r convertSfSpatial, eval = T, echo = T}
# convert sf object to Spatial
class(fwy_sf)
fwy_sf_to_sp <- as_Spatial(fwy_sf)
class(fwy_sf_to_sp)

# you can also use the function as() and add an argument "Spatial"
fwy_sf_to_sp_2 <- as(fwy_sf, "Spatial")
identical(fwy_sf_to_sp, fwy_sf_to_sp_2) # they are exactly the same

# convert Spatial object to sf
class(fwy_sp)
fwy_sp_to_sf <- st_as_sf(fwy_sp)
class(fwy_sp_to_sf)
```

### A note about `sf` and `tidyverse` {-}

As I mentioned earlier, `sf` works really well with `tidyverse`. If you are familiar with `tidyverse` then you know that one pro is the pipe (`%>%`) which lets you perform multiple functions at once without getting cluttered and hard to read. Functions for `sf` can easily be incorporated into the `tidyverse` piping method as well. For an example, we can load a csv file, covert it to an `sf` object, project it, and save it all at once using the pipes.

```{r piping, eval = F, echo = T}
read.csv("Data/Examples/Sites.csv") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>%
  st_transform(crs = 26912) %>%
  st_write(dsn = "data", layer = "Sites_shp", driver = "ESRI Shapefile")
```

Of course, this requires that you already know what the column names for the coordinates are, and this might not be the best example for a series of functions to use with the pipe. But this type of process is a useful tool to know, especially if you are already familiar with and frequently use `tidyverse`.

### Your Turn! {-}

In the attached "worksheet", there are some exerices to help you practice these concepts and functions. Try them out!

---------------------------------------------------------------------------------

## Rasters 

The primary package that deals with rasters is [`raster`](https://cran.r-project.org/web/packages/raster/index.html). Note that `raster` depends on the `sp` package, as it loads `sp` automatically when you load `raster`. ([`terra`](https://cran.r-project.org/web/packages/terra/index.html) also works with rasters but is funcitonally very similar to `raster`).

### Load a raster

When you want to load a raster that is saved in a directory, you'll use the (aptly named) function `raster`. When loading a raster that is already saved, the only argument you need is the filename (including the directory and extension)

```{r loadRaster, eval = T, echo = T}
elev <- raster("Data/Examples/elevation.tif")
elev
```
When we examine the raster (by simply calling the object like we did above), we get a lot of useful information.

* `class`: the class of the raster (this could be `RasterLayer`, `RasterStack`, or `RasterBrick`)
* `dimensions`: the number of rows, columns, and cells
* `resolution`: the size of the cells
* `extent`: the min/max x and y 
* `crs`: the coordinate reference system
* `values`: the min/max values this raster contains

I recommend *always* examining a raster after you load it in to make sure the information looks like what you would expect it to be.

You can also check all of this information with separate functions:

```{r checkRastInfo, eval = T, echo = T}
class(elev)
nrow(elev)
ncol(elev)
ncell(elev)
res(elev) # the cell resolution of the raster
extent(elev)
proj4string(elev) # alternatively, we can use crs(r) which will print out a lot more information. proj4string() is sufficient
summary(values(elev)) # put this in summary() so we can get an idea of the spread of values (rather than a list of all of the values themselves)
```

We can also plot the raster to check that it looks like what we would expect it to be

```{r plotRast, eval = T, echo = T}
plot(elev)
```

We can also plot a raster with `ggplot2`, but we need to do an extra step of converting it to a data frame first. We'll use the base R function `as.data.frame` and we need to be sure to put `xy = TRUE` in the arguments. After we do that, we can use the function `geom_raster()` within `ggplot`. Within the `aes()` function in `geom_raster()`, we need to put the columns that correspond to the x and y locations and what the fill value should represent (in this case, elevation).

```{r plotRastGGplot, eval = T, echo = T}
elev_df <- as.data.frame(elev, xy = TRUE)
head(elev_df) # the data frame of a raster contains columns for the x and y locations and the corresponding cell value
ggplot(elev_df) +
  geom_raster(aes(x = x, y = y, fill = elevation))
```
```{r plotRastGGplotPipe, eval = F, echo = T}
# You can use the pipe to run this process all at once
elev %>%
  as.data.frame(xy = TRUE) %>%
  ggplot() +
  geom_raster(aes(x = x, y = y, fill = elevation)) # This requires that you know the column name of the raster values
```

An important thing to note is that, depending on the size of the raster, it might be too computationally expensive to convert a raster to a data frame and R may give you an error.

(Another reminder that we will not cover how to make these maps look nice, this is just a quick way to check that your raster looks the way that you think it should)

#### Loading a raster with more than one band {-}

In the example above, the raster we loaded only had one band. Let's try loading in a raster downloaded from [SNODAS](https://nsidc.org/data/g02158) (Snow Data Assimilation System) which has 2 bands: SWE (snow water equivalent) and snow depth. (SNODAS offers daily rasters, and this example raster is from 02-23-2019).

```{r snow, eval = T, echo = T}
snow <- raster("Data/Examples/snow_20190223.tif")
snow
```

Notice how it says band 1 (of 2). If there are two bands, why did it only load 1? If we look in the help file for `?raster`, it says that this function creates a `RasterLayer` object, meaning only **1** band. We can also see that there's an argument called `band`, meaning you can specify which band you want to load (and the default is the first band). We *could* load individual bands one-by-one, but that's tedious and, depending how many bands a raster stack has, could be lines and lines of repetitive code. 

Instead, we'll use the function `stack()` or `brick()`

```{r snowStk, eval = T, echo = T}
snow_stack <- stack("Data/Examples/snow_20190223.tif")
class(snow_stack)
snow_stack

snow_brick <- brick("Data/Examples/snow_20190223.tif")
class(snow_brick)
snow_brick
```

When we call the stack and the brick, they look identical besides the class. So what are the differences between a stack and brick? When we look at `?brick`, the help file tells us a few differences. Both a `RasterStack` and `RasterBrick` are multi-layer raster objects, but processing time is shorter with a `RasterBrick`. However. a `RasterBrick` is less flexible than a `RasterStack` because it can only be created from a **single** file, while a `RasterStack` could be fed a list of file names or raster objects and automatically stack them all together. For the purposes of this example, a stack and a brick are essentially identical.

Let's plot the raster stack just to make sure it looks like what we might expect:

```{r plotStk, eval = T, echo = T}
plot(snow_stack)
```

Note that plotting a raster stack (or brick) will plot all the layers at once.

The benefit of working with a stack of rasters is that if you need to perform a computation on all of your rasters, you can stack them all togther and the computation will run for every raster layer in the stack. **However**, rasters will only stack together if they have the same **dimensions**, **resolution**, **extent**, and **crs**. 

For example, if we try to stack our elevation raster and stack of snow rasters together:
```{r stackError, echo = T, eval = F}
stack(elev, snow_stack)
```

`Error in compareRaster(x) : different extent`

We get an error that the extents don't match

```{r check, echo = T, eval = T}
extent(elev)
extent(snow_stack)
```

They're very close, but they need to be *exact*. **Why can't I just not stack them and run seperate computations on these rasters?** You can, but your raster cells will be slightly off from each other and may result in inaccurate results and inference.

```{r stack2, fig.cap = "A stack of rasters, showing how each cell would correspond to the ones on top and below", fig.align = 'center', out.width = '50%', echo = F}
knitr::include_graphics("pictures/raster_stack.png")
```

In a future section, we'll talk about how to make your rasters line up so you can stack them all together.

### Create a raster

To create a raster from scratch, we'll use the same function `raster()` but instead of the filename or band as arguments, we'll include the extent and resolution that we want. In this example, let's just make something random.

```{r blankRast, eval = T, echo = T}
set.seed(1) # this is to make the randomness of runif the same every time 
x <- runif(1, min = 0, max = 10)
y <- runif(1, min = 0, max = 10)
r_new <- raster(xmn = x, xmx = x + 10, ymn = y, ymx = y + 10, resolution = 1)
r_new
```
Note that because we didn't specify the CRS in the arguments, it defaulted to WGS84 lat/long. 

Right now this raster is empty. We can check with the function `values()`

```{r checkVals, eval = T, echo = T}
head(values(r_new))
```

We'll use the same function to assign fill the raster with values. If you noticed, there are 100 total cells that need to be filled, and let's fill them with random numbers.

```{r addVals, eval = T, echo = T}
values(r_new) <- runif(100)
head(values(r_new))
r_new
plot(r_new)
```

**Why would you need to create a raster from scratch?**

1. If you've written code to perform a calculation or an analysis on a raster, it's good to check that your code is doing what you think it's doing. A good way to test the code is to try it out on a "dummy" raster
2. If you're asking for help on your code (on [Stack Overflow](https://stackoverflow.com/) for example), it's good practice to either include your data (which is often very difficult to do) or to create a reproducible example. Knowing how to create fake data or fake rasters is useful to know.
3. We'll see in the next section another useful reason for creating a raster from scratch.

### Project a raster

Let's go back to the elevation raster we loaded earlier. If you noticed, the CRS of this raster is WGS84 UTM12N, as opposed to NAD83 UTM12N that we used earlier for our vector data. 

```{r checkRastProj, eval = T, echo = T}
proj4string(elev)
```

If we were going to do some sort of analysis on our vector data and raster data, we want all of our spatial objects to be in the same CRS so that we know that our layers are *truly* stacked on top of the other (for plotting and analysis). For that reason, we need to project this raster to NAD83 UTM12N. To do so, we'll use the function `projectRaster` 

```{r projR, eval = T, echo = T}
elev_proj <- projectRaster(elev, crs = "+init=epsg:26912")
elev_proj
```

Wait, let's take a look at the cell resolution again...

```{r checkRes, eval = T, echo = T}
res(elev_proj)
```

This elevation raster came from the [USGS's 3D Elevation Program](https://www.usgs.gov/3d-elevation-program). When I downloaded it, I specified a 1-arcsecond resolution (arcsecond is the unit for long/lat projections), which is equivalent to a 30mX30m resolution in a linear projection (like UTM). So why is the resolution after projecting not actually 30mX30m?

What's happening is that *every* cell needs to be projected, causing the cells to be warped and results in slightly off cell measurements. 

What if we tried specifying the cell resolution in the `projectRaster` function?

```{r proj2, echo = T, eval = T}
elev_proj_2 <- projectRaster(elev, crs = "+init=epsg:26912", res = 30)
elev_proj_2
```

We could do this. But can we be sure that all of the cells in this raster would align one-to-one in all of our other rasters? And if they don't align, can we be sure our analysis and computation on all of our rasters would be correct?

```{r proj3_temp, echo = T, eval = T}
snow_proj <- projectRaster(snow_stack, crs = "+init=epsg:26912", res = 30)
snow_proj
```

```{r stackTest, echo = T, eval = F}
stack(elev_proj_2, snow_proj)
```
`Error in compareRaster(x) : different extent`

Furthermore, there's something else happening when rasters are projected. Let's double-check the help file, `?projectRaster`. It looks like there's another argument called `method` that defaults to `"bilinear"`. If you scroll down to the details of `method`, it says it can either take `ngb` (or nearest neighbor) or `bilinear` (or bilinear interpolation). Ok...well what does that mean? Bilinear interpretation is basically a way of estimating a value in between two other values. Nearest neighbor essentially picks the exact value of a point that's closest to the focal point. 

This essentially means that when you use `projectRaster`, the raster cells are being warped and shifted slightly, so R can't just simply move the entire raster over, but it needs to know how to estimate the cell values in their new location and new resolution. So should it estimate a value in between two cell values or pick the actual value of the cell it's closest to? Either way, this will slighlty adjust the data:

```{r bilinVsNGB, echo = T, eval = T}
elev_bilinear <- projectRaster(elev, crs = "+init=epsg:26912", res = 30, method = "bilinear")
elev_ngb <- projectRaster(elev, crs = "+init=epsg:26912", res = 30, method = "ngb")

# check the spread of values in the orginal raster
summary(values(elev))

# check the spread of values in the raster estimated with bilinear 
summary(values(elev_bilinear))

# check the spread of values in the raster estimated with nearest neighbor 
summary(values(elev_ngb))
```

There's no right or wrong answer on which method you pick. The `projectRaster` help file recommends using `bilinear` for continuous rasters (like this elevation raster we're working with) and `ngb` for categorical rasters (in fact, you probably *shouldn't* use `bilinear` for categorical rasters because it will pick a value that's in between two values which aren't real numbers, but are actually stand-ins for a category). In my work I use `ngb` for continous rasters as well, so I'm going to use that for these examples, but `bilinear` is valid to use as well. (Again, as with everything, it depends on your analysis and your system.)

One thing to keep in mind with projecting rasters is that **you should not over-project and re-sample your rasters over and over again**. Imagine printing a picture and then making a copy of that printed picture. And then making a copy of the copy. And then a copy of that *that* copy. Over and over again. Eventually you end up with a warped image that only vaguely resembles the original image. A similar thing can happen if you project and re-sample your rasters even more than once. 

In contrast, there is no issue with projecting and re-projecting your vector data over and over again. 

Ok, so now that we know the dangers of projecting rasters, what **is** the best way project a raster? I recommend (and `projectRaster` also recommends) creating a template raster and using that template as a mold for all of your rasters. (*hint: this is where it comes in handy to know how to make a blank raster*)

So let's create a template raster with our desired CRS, cell resolution, and extent. To choose the cell resolution, I recommend picking your set of rasters with the smallest resolution (but keep in mind that this will affect disk space and computation time, so again there's no right or wrong answer). In this case, elevation (30mX30m) has a smaller resolution than snow (1kmX1km). The extent should be the min and max area of focus, such as your study area.

```{r createTemp, eval = T, echo = T}
# create a blank raster with desired parameters. We don't need to add any values to it
template <- raster(crs = "+init=epsg:26912", resolution = c(30, 30),
                   xmn = 228560, xmx = 238910, ymn = 4032145, ymx = 4043815)
template

# use the template to project our rasters
# from = focal raster
# to = the raster object with the desired parameters
# method = the method to compute new values 
elev_proj_template <- projectRaster(from = elev, to = template, method = "ngb")
snow_proj_template <- projectRaster(from = snow_stack, to = template, method = "ngb")
elev_proj_template
snow_proj_template

# now we can stack them! This way we know for sure that all cells align with every layer
stack <- stack(elev_proj_template, snow_proj_template)
stack
plot(stack)
```

Right now the bands' names (besides "elevation") are not very descriptive, but we can change their names so that we can more easily remember which band is which. In this case, band 1 is elevation, band 2 is SWE, and band 3 is snow depth. To change the band names, we'll use the function `names()`

```{r changeNames, eval = T, echo = T}
names(stack) <- c("elevation", "swe", "snow_depth")
stack
```

### Save a raster

To save a raster we'll use the function `writeRaster`. We need to include the raster object that we're saving, the full file name (including the directory **but not the file extension**), and the format (i.e. the file type) we want to save the raster as.

The two most common formats are `raster` (which creates two files with the extensions .gri and .grd) & `GTiff` (GeoTIFF, which creates a file with the extension .tif). I personally use GeoTIFF's, but a pro for using `raster` is that it saves the name of the raster layers that are in a stack.

(You can also check `writeFormats()` for other formats to save a raster.)

```{r saveR, eval = F, echo = T}
writeRaster(stack, "Data/Examples/elev_snow_stack", format = "GTIFF")
```

Note that some raster functions have optional arguments to add filenames and formats so that the funciton will automatically save the output after performing its function.

### Your Turn! {-}

In the attached "worksheet", there are some exerices to help you practice these concepts and functions. Try them out!

---------------------------------------------------------------------------------

Now that we know the basics of working with vector and raster data in R, let's learn how to manipulate and perform computations on these spatial objects.

<!--chapter:end:02-R_Spatial.Rmd-->

# Spatial Analysis

Now let's do some analysis with the data we've acquired already: 

* Sites point data `sites_sf`
* Utah freeways `fwy_sf_proj`

We also have some data that I've included in the exercises portion of the "worksheet": a different elevation + snow raster stack (this one is in the NW corner of Utah), a set of plots as a point feature, and a polygon feature of boundaries in Utah and who manages them:

```{r loadRast , eval = T, echo = F}
elev_snow_stk <- stack("Data/demo/elev_snow_nw_stack.tif")
names(elev_snow_stk) <- c("elevation", "swe", "snow_depth")
```

```{r showRaster, eval = T, echo = T}
elev_snow_stk
plot(elev_snow_stk)
```

```{r makePlots, eval = T, echo = F}
plots_sf <- read.csv("Data/Exercises/Plots_location.csv") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>%
  st_transform(crs = 26912)
plots_sp <- as(plots_sf, "Spatial")
```
```{r showPlots, eval = T, echo = T}
head(plots_sf)
```

```{r loadManage, eval = T, echo = T}
manage_sf <- st_read("data/Exercises/UT_land_management", "UT_land_management",
                    quiet = T) %>%
  st_transform(crs = 26912)
manage_sp <- as(manage_sf, "Spatial")
```
```{r showBound, eval = T, echo = F}
head(manage_sf)[,1:5]
manage_sf %>%
  ggplot() +
  geom_sf(aes(fill = OWNER), size = 0.1)
```

Let's plot one of the rasters with our sites point vector and Utah highways line vector. To plot just one raster layer in a stack we can either index it with double brackets or with the name:

```{r indexEx, eval = T, echo = T}
# these are different ways to get the same raster layer
elev_snow_stk[[1]]

elev_snow_stk$elevation
```

```{r projSilent, eval = T, echo = F}
fwy_sp_proj <- spTransform(fwy_sp, "+init=epsg:26912")
sites_sf_proj <- st_transform(sites_sf, 26912)
```

```{r plotAll, eval = T, echo = T}
plot(elev_snow_stk$elevation)
lines(fwy_sp_proj, lwd = 2) # lines() will plot the polyline on top of the plot (instead of drawing a new plot)
points(sites_sp_proj, pch = 16) # points() will do the same as lines() except with point data
points(plots_sp, pch = 3)
```

(This can also be done with `ggplot` using `as.data.frame` but in this case the raster may be too large for R to convert to a dataframe and plot)

Let's start on some analysis and computations that we can run on these data.

## Selecting Attributes

Perhaps you have vector data and you want to select only certain attributes or attributes that reach a focal threshold. To do so we need to set up a logical statement, and we can do this in base R or in `tidyverse`.

Let's say we want to select boundaries that are operated by BLM. In the shapefile of management boundaries, this information is located in the column "AGENCY"

```{r agency, eval = T, echo = T}
unique(manage_sf$AGENCY)
```
In base R we can use the function `which` and in `tidyverse` we can use the function `filter`

```{r selectBLM, eval = T, echo = T}
# base R
blm_boundary <- manage_sf[which(manage_sf$AGENCY == "BLM"), ] # you can do this with sp objects too 

# tidyverse
blm_boundary <- manage_sf %>% # you cannot do this with sp objects
  filter(AGENCY == "BLM")

ggplot() +
  geom_sf(data = manage_sf, col = "grey", size = 0.1) +
  geom_sf(data = blm_boundary, fill = "red", col = "grey30",
          alpha = 0.8, size = 0.1)
```

Using these functions, you can set up any logical statement using `==`, `%in%`, `>`, `>=`, `<`, `<=`, or `!` and select for the specific attributes you need.

## Select features by location

Let's make select the management boundaries based on if they are intersected by a major highway. For `sf` we'll use the function `st_intersect` and for `sp` we'll use

```{r selectIntersect, eval = T, echo = T}
manage_roads <- st_intersects(fwy_sf_proj, manage_sf) # the first argument is the target shape and the second argument the shape we're selecting from
class(manage_roads)
```
The output is an `sgbp` object, or "Sparse Geometry Binary Predicate". Basically it returns a list of vectors of integers, which refer to the indices of each polygon that intersects. 

```{r dimensions, eval = T, echo = T}
dim(manage_roads)
nrow(fwy_sf_proj)
nrow(manage_sf)
```
So the dimensions of this list are the the number of rows in the target shape (the highways) and the number of rows in the intersecting shape (the management boundaries). If we wanted to know the specifc index of a specific road that intersected with a management boundary, it would be useful to keep all of these indices seperate. Since we just want to know which boundaries intersect a road, we can collapse this whole list together.

```{r intersect, eval = T, echo = T}
manage_roads_index <- unique(unlist(manage_roads)) # just pull the unique indices
manage_roads_intersect <- manage_sf[manage_roads_index, ]

ggplot() +
  geom_sf(data = manage_sf, col = "grey", size = 0.1) +
  geom_sf(data = manage_roads_intersect, fill = "red", col = "grey30",
          alpha = 0.8, size = 0.1) +
  geom_sf(data = fwy_sf_proj, col = "black", size = 1)
```

If you look at the help file for `?st_intersects`, you'll see there are a lot of different functions that select features based on another feature.

## Joining Attributes

Let's load in a table of some data collected at each plot

```{r loadPlotData, eval = T, echo = F}
plot_data <- read.csv("data/Exercises/Plots_data.csv")
head(plot_data)
```

Let's join this table to the Plots feature so we could do some spatial analysis and mapping of the collected data. To join two tables together, we need to input the two tables and the name of the column that exists in both tables (so the join function knows how to match attributes together). In this case, that would be the Plots column. 
```{r checkPlots, eval = T, echo = T}
head(plots$Plots)
head(plot_data$Plots)
```

We can use the `tidyverse`'s `join` functions. (If you don't know how joins work, I would recommend looking at the help file by typing `?left_join` in the console)

```{r joinPlots, eval = T, echo = T}
plots_join <- left_join(plots_sf, plot_data, by = "Plots")
head(plots_join)
```

Great! At this point you could then do some spatial analysis based on location, or make a map based on average biomass, for example. However, that's outside the scope of this workshop.

Joining two tables together is a valuable tool to know, not just for GIS but for any data management.

## Cropping

### Cropping a vector {-}

If you noticed earlier, the highway polyline runs outside of the elevation raster. What if we want to crop the vector so that it falls only within the raster?

For `sf` we'll use the function `st_crop` (which requires an object of class `sf` or `sfc` and the min/max x & y extent we want to crop the feature to). For `sp` we'll use the function `crop`. (`crop` is actually in the `raster` package, but remember that `raster` is dependent on `sp`? That means that some `raster` functions can be used on `Spatial*` objects too). For `crop` we need the object we're cropping and the extent we're cropping to (or an object that an extent can be derived from, in this case the raster itself)

```{r cropRoads, eval = T, echo = T, warning = F}
# sf:
# First we need the extent of the raster that is compatible with sf. For that we'll use st_bbox()
rast_ext <- st_bbox(extent(elev_snow_stk))
rast_ext
fwy_crop_sf <- st_crop(fwy_sf_proj, rast_ext)

# sp:
fwy_crop_sp <- crop(fwy_sp_proj, elev_snow_stk)
```

```{r checkCrop, eval = T, echo = F}
plot(elev_snow_stk$elevation)
lines(fwy_crop_sp, lwd = 2)
```

### Cropping a raster {-}

We can also easily crop a raster. Let's say we wanted to crop our raster stack down to only the area around our site that's in the Uintas

```{r plotSites, eval = T, echo = T, fig.cap = "the Uintas are that high-elevation mountain range in the middle right of this map", fig.align = 'center'}
plot(elev_snow_stk$elevation)
points(sites_sp_proj, pch = 16)
```

First we need to find out what site number that is. We'll use the `text()` function. There is a `text()` function for base R plotting, but the `raster` package adapted that function to plot text from rasters and `Spatial*` objects. Let's use that function, so we need to specify which package it comes from using `raster::`

```{r checkSiteN, eval = T, echo = T}
plot(elev_snow_stk$elevation)
raster::text(sites_sp_proj, labels = "Site", halo = T)
```

Site 8! Let's filter our spatial data to just this site. 

```{r filterSite8, eval = T, echo = T}
# base R
site_8 <- sites_sf_proj[which(sites_sf_proj$Site == 8), ] # remember that you can use this method for sp objects too

# tidyverse
site_8 <- sites_sf_proj %>%
  filter(Site == 8)

site_8
```

But we don't want to crop the raster down to a single point, so let's first make a buffer (5kmX5km) around this specific site. We'll use `st_buffer()` to do so.

```{r buffer, eval = T, echo = T}
buffer <- st_buffer(site_8, dist = 5000) # units are in meters
buffer

ggplot() +
  geom_sf(data = buffer) +
  geom_sf(data = site_8, col = "red", size = 2) +
  coord_sf(datum = st_crs(26912)) # this plots the axes to UTM coordinates instead of latlong coordinates
```

To crop a raster, we'll use the same function we used to crop a `Spatial*` object: `crop`. Remember that I said earlier that any function we perform on a stack of rasters will run for every raster in that stack!

Earlier when we used `crop`, we could just put in the object itself and the function would automatically crop to the extent of that object. But that only works for objects of class `Raster*`, `Spatial*`, or `Extent`. Because our buffer is of class `sf`, we can't just put the object itself in. Instead we need to put in its extent (*or* you could convert the buffer to a `Spatial*` object)

```{r cropRast, eval = T, echo = T}
stack_crop <- crop(elev_snow_stk, extent(buffer))
stack_crop
plot(stack_crop)
```

```{r checkCropR, eval = T, echo = F}
buff_sp <- as(buffer, "Spatial")
plot(stack_crop$elevation)
lines(buff_sp, lwd = 2)
raster::text(sites_sp_proj, labels = "Site", halo = T)
```

## Extract Raster Values

What if we need to get data from our rasters at our specific site locations? We can use the function `extract()`.

Let's load a landcover raster so we can classify the habitat types of our sites

```{r landcover, eval = T, echo = T}
landcover <- raster("Data/Examples/landcover.tif")
landcover
plot(landcover)
raster::text(sites_sp_proj, labels = "Site", halo = T)
```

`extract` returns a vector whose indices match the indices of the spatial object. We could leave it as a vector, or we could automatically attach it to the dataframe using `$`

```{r extract, eval = T, echo = T}
sites_sf_proj$land_value <- raster::extract(landcover, sites_sp_proj)
sites_sf_proj
```

Ok but what do these numbers mean? Our landcover raster is a categorical raster, so these numbers aren't actually real numbers but represent a habitat type. Fortunately we have a dataframe indicating what these numbers mean.

```{r landInfo, eval = T, echo = T}
land_info <- read.csv("Data/Examples/landcover_info.csv")
head(land_info)[,1:5]
```

The column "Value" corresponds to the cell value we extracted from the raster. We can use what we learned earlier how to join two tables together, but first we need to make sure the ID column ("Value") for both tables are named the same.

```{r join, eval = T, echo = T}
sites_sf_land <- sites_sf_proj %>%
  rename(Value = land_value) %>% # rename the column so it matches in both tables
  left_join(land_info, by = "Value") # join by the column "Value"
head(sites_sf_land)[,1:6]
```
```{r plotLandPoints, eval = T, echo = F}
sites_sf_land %>%
  ggplot() +
  geom_sf(data = utah) +
  geom_sf(aes(col = ClassName), size = 2)
```

Awesome!

## Distance

Let's say we needed to know how far from a major road each of our sites are. We'll use the function `st_distance` for our `sf` objects. We simply need to input the focal feature (the sites) and the feature 

```{r distance, eval = T, eval = T}
dist <- st_distance(sites_sf_proj, fwy_sf_proj)
dim(dist)
```

What did this do? Why are there so many columns? Remember that our Utah highways feature is a **poly**line, meaning it's a line of lines. If we look at the dimensions of the highways feature:

```{r check dim, eval = T, echo = T}
nrow(fwy_sf_proj)
```

There are **1849** lines (i.e. roads) that make up this whole feature. So `st_distance` found the distance for each site (the number of rows) for *every* road (the number of columns). This *could* be useful information, but presumably we want just the distance of the *closest* road. To find that distance, we'll have to do some data frame manipulation.

```{r findShortestDist, eval = T, echo = T}
dist_df <- as.data.frame(dist)
dist_df[1:5, 1:5]

colnames(dist_df) <- fwy_sf_proj$UNIQUE_ID
dist_df <- dist_df %>%
  mutate(Site = sites_sf_proj$Site) %>% # add a column for Sites
  relocate(Site, .before = colnames(dist_df)[1]) # move to the front of the dataframe
dist_df[1:5, 1:5]

dist_df <- dist_df %>%
  pivot_longer(cols = -Site, names_to = "Road_Name")
head(dist_df)

dist_df <- dist_df %>%
  group_by(Site) %>%
  mutate(Distance = min(value)) %>%
  filter(value == Distance) %>%
  dplyr::select(-value)
head(dist_df)
```

We could then join this information to our Sites feature

```{r joinDistSites, eval = T, echo = T}
sites_sf_proj <- left_join(sites_sf_proj, dist_df, by = "Site")
head(sites_sf_proj)
```

(Note that if you look at the help file i.e. `?st_distance`, there are other functions to calculate geometric measurements for `sf` objects: `st_area` and `st_length`)


## Raster Cell Stats

In my research I often have to perform cell algebra or focal statistics. Maybe you need to know the average elevation or the total herbaceous biomass within a certain area. The way to get these values are with the function `cellStats`. We simply need to input the raster and the `stat` function: sum, mean, min, max, sd, or a homemade function. Let's say we need to calculate the average elevation, SWE, and snow depth within the buffer we made earlier. 

```{r cellStats, eval = T, echo = T}
cellStats(stack_crop, stat = "mean")
```

Note that there's an option to include `na.rm` in the arguments. `na.rm = TRUE` is the default.

## Calculate Terrain Characteristics 

From a DEM (digital elevation model) we can obtain a lot of other rasters that are likely useful in GIS research. The elevation raster we've been working with is a DEM. From a DEM we can derive other terrain characteristics : 

* Slope: Measurement of "steepness"
* Aspect: Measurements of "Northness" and "Eastness"
* Flow direction of water: the direction of the greatest drop in elevation
* Terrain Ruggedness Index (TRI): the mean of the absolute differences between the value of a cell and the value of its 8 surrounding cells
* Topographic Position Index (TPI): the difference between the value of a cell and the mean value of its 8 surrounding cells
* Roughness: the difference between the maximum and the minimum value of a cell and its 8 surrounding cells

These definitions came from the help file for the function we can use to derive these characteristics: `terrain()`. 

```{r terrain, eval = F, echo = T}
slope <- terrain(elev_snow_stk$elevation, opt = "slope", unit = "radians")
aspect <- terrain(elev_snow_stk$elevation, opt = "aspect", unit = "radians")
roughness <- terrain(elev_snow_stk$elevation, opt = "roughness")
terrain_stk <- stack(elev_snow_stk$elevation, slope, aspect, roughness)
terrain_stk
```
```{r loadStk, eval = T, echo = F}
terrain_stk <- stack("Data/demo/dem/terrain_stk.tif")
names(terrain_stk) <- c("elevation", "slope", "aspect", "flowdir", "TPI", "TRI", "roughness")
terrain_stk_2 <- stack(terrain_stk$elevation, terrain_stk$slope, terrain_stk$aspect, terrain_stk$roughness)
terrain_stk_2
plot(terrain_stk_2)
```

To compute the Northness or Eastness of a cell, we actually have to do one more step to the aspect raster. Aspect is a circular measurement (which is why its units are in degrees or radians), so (if you remember how trigonometry works) to calculate northness and eastness we need to use cosine and sine respectively. Because our units are in radians, we can simply apply the `cos()` and `sin()` functions directly to the aspect raster.

```{r cosSine, eval = T, echo = T}
aspect_cos <- cos(terrain_stk$aspect)
aspect_sin <- sin(terrain_stk$aspect)
aspect_stk <- stack(aspect_cos, aspect_sin)
aspect_stk
plot(aspect_stk)
```

## A Note About Loops

Learning all these functions is all well and good, but what if you have to perform them all on multiple features or rasters? Of course, you can always copy and paste, but that can soon become confusing and messy and cause your code to be inefficient. The better way to address this is with loops! `for` loops and `lapply` are lifesavers and I use them in all of my code. A previous workshop went into more depth on how to use loops, so I won't go over them in too much detail. But I do want to show some ways you can use them for GIS applications.

(These code chunks are for demonstration only, these data and directories don't actually exist)

```{r loopEx, eval = F, echo = T}
# Example 1: Load a set of shapefiles and find the area for each
filenames <- list.files(dir) # get a list of shapefile names in a directory

area_list <- c() # create an empty vector for the areas to live in 

for(i in 1:length(filenames)){
  # load the shapefile
  shp <- st_read(filenames[i])
  
  # calculate the area
  area <- st_area(shp)
  
  # put the area into the vector
  area_list <- c(area_list, area)
}

# -----------------------------------------------------------------------------X

# Example 2: Load a set of shapefiles, generate a buffer for each, and calculate the  #            mean value of a raster within that buffer and the focal feature

filenames <- list.files(dir) # get a list of shapefile names in a directory
r <- raster(raster_filename) # load a raster

lapply(filenames, function(fn){
  # load a shapefile
  shp <- st_read(fn)
  
  # generate a 10kmX10km buffer 
  buffer <- st_buffer(shp, dist = 10000)
  
  # crop the raster to the shape and the buffer
  r_shp <- crop(r, extent(shp))
  r_buffer <- crop(r, extent(buffer))
  
  # calculate the mean value of the raster within the buffer and the feature
  r_shp_mean <- cellStats(r_shp, stat = "mean", na.rm = TRUE)
  r_buff_mean <- cellStats(r_shp, stat = "mean", na.rm = TRUE)
  
  # return both means in a list
  return(list(r_shp_mean, r_buff_mean))
})

# -----------------------------------------------------------------------------X

# Example 3: Generate a raster of the sum from a set of RasterStacks
#            and then save the output raster
filenames <- list.files(dir) # get a list of raster files in a directory
out_names <- paste0(filenames, "_sum")

lapply(1:length(filenames), function(i){
  # load RasterStak
  stk <- stack(filenames[i])
  
  # create a raster that is the sum of all layers in the stack
  sum <- calc(stk, fun = sum)
  sum <- sum(stk) # these two operations are equivalent
  
  writeRaster(sum, out_names[i], format = "GTiff")
})

# -----------------------------------------------------------------------------X

# Example 4: Pull the number of zeros in a set of rasters
filenames <- list.files(dir) # get a list of raster files in a directory
lapply(filenames, function(fn){
  # load raster
  rast <- raster(fn)
  
  # get the number of zeros in the raster
  n_0 <- getValues(rast) == 0 %>%
      which() %>%
      length()
  return(n_0)
})

```

Even more efficient would be to run these in parallel, but that is way beyond the scope of this workshop

---------------------------------------------------------------------------------

I hope these functions helped you! The next chapter goes over some ways of obtaining the data we worked on today.


<!--chapter:end:03-Spatial_analysis.Rmd-->

# Where to Obtain Data & Further Resources

## Data Sources

### `maps` package

There is package you can install called [`maps`](https://www.rdocumentation.org/packages/maps/versions/3.4.0) that contains a lot of global and national features (such as state boundaries, bodies of water, etc). These features don't contain a lot of data or attributes, so they are more useful for making maps and not so much for spatial analysis. However, I've used them to pull state boundaries to get the extents I need for cropping rasters, for example.

The [help documentation](https://www.rdocumentation.org/packages/maps/versions/3.4.0) for this package and [this website](https://www.rdocumentation.org/packages/maps/versions/3.4.0) show you how you can access different features from this package. I'll show you briefly how I use it to pull the state boundary of Utah:

```{r utahMaps, eval = T, echo = T}
utah <- maps::map("state", plot = F, fill = TRUE) %>%
  # turn into sf obj
  sf::st_as_sf() %>%
  # pull out utah 
  dplyr::filter(ID == "utah")
utah
plot(st_geometry(utah))
```

### Utah GIS

Many states and cities have GIS data that are publically available. To find them, typically I google something like "Utah GIS data" or the specific data I'm looking for. Utah helpfully has a website called [gis.utah.gov](https://gis.utah.gov/#data). Some data they have that could be useful for ecological research are [Geoscience](https://gis.utah.gov/data/geoscience/), [Water](https://gis.utah.gov/data/water/), and [Bioscience](https://gis.utah.gov/data/bioscience/), just to name a few

The highway feature and the management boundaries feature we've been working on in this workshop came from this website on their [Transportation page](https://gis.utah.gov/data/transportation/) and [Land Ownership page](https://gis.utah.gov/data/cadastre/land-ownership/) respectively

### Other Environmental Rasters

In this workshop we worked with a [DEM (digital elevation model)]((https://www.usgs.gov/3d-elevation-program)) and [snow data]((https://nsidc.org/data/g02158). In my own research I work with these data frequently. I also work with [RAP (Rangeland Analysis Platform)](https://rangelands.app/), which offers biomass and vegetation cover, and [NDVI (Normalized Difference Vegetation Index)](https://modis.gsfc.nasa.gov/data/dataprod/mod13.php), which reports an index of vegetation "green-ness". Many of these rasters can be downloaded from the website's user-interface, **but** they can also be downloaded directly from R!

This can be a bit complicated if you've never done it before, but fortunately Brian Smith, a PhD in WILD at USU, has already written a [guide](https://bsmity13.github.io/spatial_covars/)!

My research is very temporally-dependent, and many of these rasters are available daily, weekly, or yearly. It can get very tedious to repeat lines and lines of code to download a year's worth of daily rasters. This is, again, where loops come in handy!

In this workshop we also worked with a Landcover/Landuse raster. I obtained that from [here](https://www.usgs.gov/programs/gap-analysis-project/science/land-cover-data-download)

## More Resources

This workshop really just touches the surface on how to work with GIS in R. There are a plethora of resources online to help you enhance what you learned today and help you solve your particular GIS problem. Here are just a few:

* https://cengel.github.io/R-spatial/
* https://www.bookdown.org/mcwimberly/gdswr-book/ 
* https://www.jessesadler.com/tags/gis/
* https://geocompr.robinlovelace.net/
* [GIS stack exchange](https://gis.stackexchange.com/)
* Google!

### Learn more about GIS in general

* https://docs.qgis.org/3.22/en/docs/index.html
* https://wiki.gis.com/
* https://gisgeography.com/ 
* https://www.gislounge.com/
* http://innovativegis.com/basis/primer/The_GIS_Primer_Buckley.pdf

## Acknowledgements

This workshop is partially adapted from [Claudia Engel](https://cengel.github.io/R-spatial/)

This website was created with [R bookdown](https://bookdown.org/yihui/bookdown/)

### Figure Citations

[Figure 2.4](https://docs.qgis.org/3.22/en/docs/gentle_gis_introduction/coordinate_reference_systems.html)

[Figure 2.5](https://gisgeography.com/utm-universal-transverse-mercator-projection/)

[Figure 2.6](https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system)

[Figure 2.13](https://gisgeography.com/spatial-data-types-vector-raster/)


<!--chapter:end:04-Data-and-Resources.Rmd-->

